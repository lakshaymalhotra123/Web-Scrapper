{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief Description of the Assignment:\n",
    "\n",
    "This Web Scrapper is extracting data from multiple sources, 'List_of_United_States_cities_by_population' being the primary Source page. Also, this Scrapper is extracting for all City Infobox geophcard vcard details.\n",
    "\n",
    "There are total 315 (1 primary and 314 City links) sites from which data is being gathered, cleaned and stored into 7 separate CSV files. There is one big CSV files which contains all city details and all other 6 files provides insights on overall picture of Cities of US by population.\n",
    "\n",
    "This Scrapper can be used to extract lot of of information besides its development scope with little modification.\n",
    "\n",
    "ReadMe:\n",
    "\n",
    "1) Please install all the packages that are pre-installed in designated system\n",
    "\n",
    "2) CSV Files Path Shoulde be changed to as per the User speicification before running to_csv commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import re\n",
    "import warnings\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population'\n",
    "html = requests.get(url).content\n",
    "df_list = pd.read_html(html)\n",
    "df1 = df_list[-1]\n",
    "df2 = df_list[-2]\n",
    "df3 = df_list[-4]\n",
    "df4 = df_list[-6]\n",
    "df5 = df_list[-7]\n",
    "df6 = df_list[-8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = np.array(df1.iloc[:1])\n",
    "for i in range(len(list1[0])):\n",
    "    df1 = df1.rename(index=int,columns = {i:list1[0][i]})\n",
    "df1 = df1.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop([6, 8], axis=1)\n",
    "df2 = df2.rename(index = int,columns = {9:'ANSI',10:'Location'})\n",
    "list2 = np.array(df2.iloc[:1])\n",
    "for i in range(len(list2[0])):\n",
    "    df2 = df2.rename(index=int,columns = {i:list2[0][i]})\n",
    "df2 = df2.iloc[1:]\n",
    "df2.columns = df2.columns.fillna('2010 popultaion density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.drop([6,8],axis=1)\n",
    "list3 = np.array(df3.iloc[:1])\n",
    "df3 = df3.rename(index = int,columns = {9:'Location'})\n",
    "for i in range(len(list3[0])):\n",
    "    df3 = df3.rename(index=int,columns = {i:list3[0][i]})\n",
    "df3 = df3.iloc[1:]\n",
    "df3.columns = df3.columns.fillna('2017 popultaion density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "list4 = np.array(df4.iloc[:1])\n",
    "for i in range(len(list4[0])):\n",
    "    df4 = df4.rename(index=int,columns = {i:list4[0][i]})\n",
    "df4 = df4.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "list5 = np.array(df5.iloc[:1])\n",
    "for i in range(len(list5[0])):\n",
    "    df5 = df5.rename(index=int,columns = {i:list5[0][i]})\n",
    "df5 = df5.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df6.drop([7,9],axis=1)\n",
    "list6 = np.array(df6.iloc[:1])\n",
    "df6 = df6.rename(index = int,columns = {10:'Location'})\n",
    "for i in range(len(list6[0])):\n",
    "    df6 = df6.rename(index=int,columns = {i:list6[0][i]})\n",
    "df6 = df6.iloc[1:]\n",
    "df6.columns = df6.columns.fillna('2016 popultaion density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup((requests.get(url)).text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = soup.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ([i.text for i in row[19].find_all(\"th\")])\n",
    "header = [i.strip() for i in header]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for i in range(20,334):\n",
    "    links.append('https://en.wikipedia.org'+ row[i].find('a')['href'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all(the_list, val):\n",
    "        while val in the_list:\n",
    "            the_list.remove(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addRow(df,ls):\n",
    "    df.loc[len(df)] = ls\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "def City_Scrapper(city_soup): \n",
    "    \n",
    "    t= 100\n",
    "    try:\n",
    "        for i in range(len(city_soup.find_all('table'))): \n",
    "            if city_soup.find_all('table')[i]['class'] == ['infobox', 'geography', 'vcard']:\n",
    "                t = i\n",
    "                break\n",
    "    except: \n",
    "        for i in range(i+1,len(city_soup.find_all('table'))): \n",
    "            if city_soup.find_all('table')[i]['class'] == ['infobox', 'geography', 'vcard']:\n",
    "                t = i\n",
    "                break\n",
    "    #### Finding the starting point for scrapper to start ####\n",
    "    for j in range(len(city_soup.find_all('table')[t].find_all('tr'))):\n",
    "        if city_soup.find_all('table')[t].find_all('tr')[j].contents[0].text == 'Country':\n",
    "            start = j\n",
    "            break\n",
    "    for j in range(len(city_soup.find_all('table')[t].find_all('tr'))):\n",
    "        if city_soup.find_all('table')[t].find_all('tr')[j].contents[0].text == 'Website':\n",
    "            end = j+1\n",
    "            break\n",
    "    table = city_soup.find_all('table')[t].find_all('tr')[start:end]\n",
    "    \n",
    "    #### Extracting the columns for DataFrame ####\n",
    "    col= []\n",
    "    value = []\n",
    "    try:\n",
    "        for i in range(0,len((table))):\n",
    "            if len(table[i].contents) == 1:\n",
    "                value = table[i].th.contents[0]\n",
    "                col.append(['SPLITTER'])\n",
    "                continue\n",
    "            elif '•' in table[i].contents[0].text: \n",
    "                col.append([(value+table[i].contents[0].text).replace('•',' ')])\n",
    "            elif len(table[i].contents) > 1 and '•' not in table[i].contents[0]:\n",
    "                col.append([table[i].contents[0].text])\n",
    "    except Exception as e:\n",
    "        print(logger.exception(e))\n",
    "    col = reduce(lambda x,y:x+y,col)\n",
    "    col = [w.replace('\\xa0', ' ') for w in col]\n",
    "    col = [re.sub('((\\[)[0-9]*\\w*(\\]))','',w) for w in col]\n",
    "    col = [w.replace('\\n', ' ') for w in col]\n",
    "    remove_all(col, 'SPLITTER')\n",
    "    \n",
    "     #### Extracting the Data for DataFrame ####\n",
    "    row= []\n",
    "    try:\n",
    "        for i in range(0,len((table))):\n",
    "            if len(table[i].contents) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                row.append([table[i].contents[1].text])\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    row = reduce(lambda x,y:x+y,row)\n",
    "    row = [w.replace('\\xa0', ' ') for w in row]\n",
    "    row = [re.sub('((\\[)[0-9]*\\w*(\\]))','',w) for w in row]\n",
    "    row = [w.replace('\\n', ' ') for w in row]\n",
    "    return(col,row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning Columns\n",
    "def cleaning(uncleaned_col):\n",
    "    uncleaned_col = [w.replace('(s)', ' ') for w in uncleaned_col]\n",
    "    uncleaned_col = [w.upper() for w in uncleaned_col]\n",
    "    uncleaned_col = [w.strip() for w in uncleaned_col]  \n",
    "    return(uncleaned_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Storing all parsed data into one list\n",
    "df = []\n",
    "for city_link in links:\n",
    "    city_soup = BeautifulSoup((requests.get(city_link)).text,'html.parser')\n",
    "    \n",
    "    mylist = cleaning(City_Scrapper(city_soup)[0])\n",
    "    counts = Counter(mylist) \n",
    "    for s,num in counts.items():\n",
    "        if num > 1: # ignore strings that only appear once\n",
    "            for suffix in range(1, num + 1): # suffix starts at 1 and increases by 1 each time\n",
    "                mylist[mylist.index(s)] = s + str(suffix) # replace each appearance of s\n",
    "                \n",
    "    City_df = pd.DataFrame()\n",
    "    for i in range(len(mylist)):\n",
    "        City_df[mylist[i]] = mylist[i]\n",
    "    addRow(City_df,City_Scrapper(city_soup)[1])\n",
    "    df.append(City_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding City to DataFrame\n",
    "df6['City'] =[re.sub('((\\[)\\w*])','',w) for w in df6['City']]\n",
    "for i in range(len(df)):\n",
    "    df[i]['City'] = df6['City'][i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting final Column for csv\n",
    "columns = []\n",
    "clean_col= []\n",
    "uni_col = []\n",
    "for i in range(len(df)-1):\n",
    "    columns.append((list(df[i].columns)+list(df[i+1].columns)))\n",
    "uni_col = set(sum(columns,[]))\n",
    "uni_col = list(uni_col)\n",
    "for i in range(1,len(uni_col)):\n",
    "    if uni_col[i][-1] == 'S' and uni_col[i][0:-1] in uni_col:\n",
    "        clean_col.append([uni_col[i][0:-1]])\n",
    "    else:\n",
    "        clean_col.append([uni_col[i]])\n",
    "clean_col = set(sum(clean_col,[]))\n",
    "clean_col = list(clean_col)\n",
    "clean_col = [re.sub('((\\[)+)','',w) for w in clean_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping unecessary columns\n",
    "for i in range(len(df)):\n",
    "    for l in df[i].columns:\n",
    "        if l== '':\n",
    "            df[i].drop('', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning Columns in DataFrame\n",
    "for i in range(len(df)):\n",
    "    for l in df[i].columns:\n",
    "        l = re.sub('([(\\[))(\\]])','',l)\n",
    "        df[i]=df[i].rename(columns={l: l})\n",
    "        if l[-1] == 'S' and l[0:-1] in sum(columns,[]):\n",
    "            df[i]=df[i].rename(columns={l: l[0:-1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning Columns in other DataFrames\n",
    "df1 = df1.rename(columns={w:re.sub('((\\[)\\w*])','',w) for w in df1.columns})\n",
    "df2 = df2.rename(columns={w:re.sub('((\\[)\\w*])','',w) for w in df2.columns})\n",
    "df3 = df3.rename(columns={w:re.sub('((\\[)\\w*])','',w) for w in df3.columns})\n",
    "df4 = df4.rename(columns={w:re.sub('((\\[)\\w*])','',w) for w in df4.columns})\n",
    "df5 = df5.rename(columns={w:re.sub('((\\[)\\w*])','',w) for w in df5.columns})\n",
    "df6 = df6.rename(columns={w:re.sub('((\\[)\\w*])','',w) for w in df6.columns})\n",
    "df1['2018 estimated population'] =[re.sub('((\\[)\\w*])','',w) for w in df1['2018 estimated population']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating final DataFrame\n",
    "final_df = pd.DataFrame()\n",
    "for i in range(len(clean_col)):\n",
    "    final_df[clean_col[i]] = clean_col[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Data in final DataFrame\n",
    "for i in range(len(df)):\n",
    "    final_df = final_df.append(df[i], ignore_index=True)\n",
    "final_df = final_df.dropna(how='all', axis=1)\n",
    "final_df = final_df.rename(columns={w:re.sub('([(\\[))(\\]])','',w) for w in final_df.columns})\n",
    "final_df = final_df.replace(np.nan, '', regex=True)\n",
    "final_df = final_df.drop(['CITY','APPROVED','A CITY'],axis=1)\n",
    "final_df['AREA CONSOLIDATED CITY-COUNTY'] = [\"{}{}\".format(b_, a_) for a_, b_ in zip(list(final_df['AREA   CONSOLIDATED CITY-PARISH']), list(final_df['AREA   CONSOLIDATED CITY-PARISHN 1']))]\n",
    "final_df['AREA CONSOLIDATED CITY-COUNTY'] = [\"{}{}\".format(b_, a_) for a_, b_ in zip(list(final_df['AREA   CONSOLIDATED CITY-COUNTY']), list(final_df['AREA   CONSOLIDATED CITY–COUNTY']))]\n",
    "final_df['AREA TOTAL'] = [\"{}{}\".format(b_, a_) for a_, b_ in zip(list(final_df['AREA   TOTAL']), list(final_df['AREA   TOTALN 2']))]\n",
    "final_df = final_df.drop(['AREA   CONSOLIDATED CITY-PARISH','AREA   CONSOLIDATED CITY-PARISHN 1',\\\n",
    "                          'AREA   CONSOLIDATED CITY-COUNTY','AREA   CONSOLIDATED CITY–COUNTY',\\\n",
    "                         'AREA   TOTAL','AREA   TOTALN 2'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('A:\\Crawler df/Cityover10k.csv')\n",
    "df2.to_csv(\"A:\\Crawler df/CDP's.csv\")\n",
    "df3.to_csv(\"A:\\Crawler df/Puerto Rico's municipalities.csv\")\n",
    "df4.to_csv(\"A:\\Crawler df/State and City Nos.csv\")\n",
    "df5.to_csv(\"A:\\Crawler df/Pop vs Number of municipal governments.csv\")\n",
    "df6.to_csv(\"A:\\Crawler df/City Details.csv\")\n",
    "final_df.to_csv('A:\\Crawler df/City Infobox.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
